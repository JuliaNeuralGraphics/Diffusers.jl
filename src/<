"""
    load_pretrained_model(model_name::String, config::String, bin_file::String)

Loads config and model state dict from HuggingFace library.
For example:
```julia
julia> state_dict, cfg = load_pretrained_model("runwayml/stable-diffusion-v1-5", "unet/config.json", "unet/diffusion_pytorch_model.bin")
```
"""
function load_pretrained_model(
    model_name::String; state_file::String, config_file::String,
)
    config = load_hgf_config(model_name; filename=config_file)
    state_url = HuggingFaceURL(model_name, state_file)
    state = Pickle.Torch.THload(_hgf_download(state_url))
    state_dict_to_namedtuple(state), config
end

function load_hgf_config(model_name::String; filename::String)
    url = HuggingFaceURL(model_name, filename)
    JSON3.read(read(_hgf_download(url)))
end

function _hgf_download(
    url::HuggingFaceURL; cache::Bool = true,
    auth_token = HuggingFaceApi.get_token(),
)
    hf_hub_download(
        url.repo_id, url.filename; repo_type=url.repo_type,
        revision=url.revision, auth_token, cache)
end

function state_dict_to_namedtuple(state_dict)
    ht = Pickle.HierarchicalTable()
    foreach(((k, v),) -> setindex!(ht, v, k), pairs(state_dict))
    _ht2nt(ht)
end

_ht2nt(x::Some) = something(x)
_ht2nt(x::Pickle.HierarchicalTable) = _ht2nt(x.head)
function _ht2nt(x::Pickle.TableBlock)
    if iszero(length(x.entry))
        return ()
    else
        tks = Tuple(keys(x.entry))
        if all(Base.Fix1(all, isdigit), tks)
            n_indices = maximum(parse.(Int, tks)) + 1
            inds = Vector(undef, n_indices)
            foreach(tks) do is
                i = parse(Int, is) + 1
                inds[i] = _ht2nt(x.entry[is])
            end
            return inds
        else
            cs = map(_ht2nt, values(x.entry))
            ns = map(Symbol, tks)
            return NamedTuple{ns}(cs)
        end
    end
end

# TODO verify keys before loop and fail early

function load_state!(layer::Flux.Conv, state)
    for k in keys(state)
        v = getfield(state, k)
        if k == :weight
            # BCHW -> WHCB & flip kernel from cross-correlation to convolution.
            v = permutedims(v, (4, 3, 2, 1))[end:-1:1, end:-1:1, :, :]
        end
        getfield(layer, k) .= v
    end
end

function load_state!(layer::Flux.Dense, state)
    for k in keys(state)
        getfield(layer, k) .= getfield(state, k)
    end
end

function load_state!(chain::Flux.Chain, state)
    for (i, layer) in enumerate(chain)
        (layer isa Dropout || layer ≡ identity) && continue
        load_state!(layer, state[i])
    end
end

function load_state!(layer::Flux.LayerNorm, state)
    for k in keys(state)
        key = getfield(layer.diag, k == :weight ? :scale : k)
        val = getfield(state, k)
        key .= val
    end
end

function load_state!(layer::Flux.GroupNorm, state)
    layer.γ = state.weight
    layer.β = state.bias
    return nothing
end

function load_state!(attn::Attention, state)
    if cross_attention(attn)
        @show keys(state)
        for k in keys(state)
            # TODO handle norm
            load_state!(getfield(attn, k == :group_norm ? :norm : k), getfield(state, k))
        end
    else
        # TODO
    end
end

function load_state!(fwd::FeedForward, state)
    load_state!(fwd.fn[1], state.net[1].proj)
    load_state!(fwd.fn[4], state.net[3])
end

function load_state!(block::TransformerBlock, state)
    load_state!(block.attention_1, state.attn1)
    (:attn2) in keys(state) && load_state!(block.attention_2, state.attn2)

    load_state!(block.fwd, state.ff)
    load_state!(block.norm_1, state.norm1)
    load_state!(block.norm_2, state.norm2)
    load_state!(block.norm_3, state.norm3)
end

function load_state!(tr::Transformer2D, state)
    for k in keys(state)
        load_state!(getfield(tr, k), getfield(state, k))
    end
end

function load_state!(block::ResnetBlock2D, state)
    load_state!(block.init_proj[1], state.norm1)
    load_state!(block.init_proj[2], state.conv1)
    load_state!(block.out_proj[3], state.conv2)
    load_state!(block.norm, state.norm2)

    :time_emb_proj in keys(state) && load_state!(
        block.time_emb_proj[2], state.time_emb_proj)

    (block.conv_shortcut ≡ identity) || load_state!(
        block.conv_shortcut, state.conv_shortcut)
end

function load_state!(tr::CrossAttnDownBlock2D, state)
    for k in keys(state)
        if k == :downsamplers
            load_state!(getfield(tr, k)[1], getfield(state, k)[1].conv) # inside .conv
        else
            load_state!(getfield(tr, k), getfield(state, k)) 
        end
    end
end

function load_state!(tr::CrossAttnMidBlock2D, state)
    for k in keys(state)
        load_state!(getfield(tr, k), getfield(state, k))
    end
end

function load_state!(vae::AutoencoderKL, state)
    for k in keys(state)
        load_state!(getfield(vae, k), getfield(state, k))
    end
end

function load_state!(enc::Encoder, state)
    load_state!(enc.conv_in, state.conv_in)
    load_state!(enc.conv_out, state.conv_out)
    load_state!(enc.norm, state.conv_norm_out)
    @show keys(state.mid_block.attentions[1])
    # TODO MidBlock
    # TODO mid block & down blocks
    @show keys(state)
end

function load_state!(enc::Decoder, state)
    @show keys(state)
end

# TODO Encoder
# TODO Decoder
# TODO Downsample2D
# TODO Upsample2D
# TODO SamplerBlock2D

load_state!(::Flux.Dropout, _) = return

load_state!(::Nothing, _) = return
